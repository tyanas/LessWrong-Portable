<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html lang="en-US">
<head>
	<meta http-equiv="content-type" content="text/html; charset=UTF-8" />
	<link rel="canonical" href="https://www.readthesequences.com/Bayesians-Vs-Barbarians" />
	<title>Bayesians vs. Barbarians  </title>
	<meta name="viewport" content="width=device-width, initial-scale=1"/>
	<link rel='stylesheet' href='https://www.readthesequences.com/wiki/pub/skins/readthesequences/skin.css' type='text/css' />
	<!--HTMLHeader--><style type='text/css'><!--.TCprogress {background-color:#5af; min-height:13px; width:13px; color:#fff; margin-left:auto; margin-right:auto;}
table.TotalCounter td {font-size:x-small; text-align:center}a[href^='http://archive.is/timegate/'] { opacity: 0.5;  }

.footnote_block_begin {
	width: 160px;
	border-bottom: 1px solid blue;
	margin-bottom: 0.5em;
}
div.footnote {
	margin: 0 3em 0.5em 0;
	padding-left: 2em;
	font-size: 0.9em;
	position: relative;
}
div.footnote .footnote-number {
	position: absolute;
	left: 0;
	width: 1.5em;
	text-align: right;
}
div.footnote .footnote-number::after {
	content: '.';
}
.num { position: relative; font-size: 0.7em; bottom: 0.5em; right: 0.1em; margin-left: 0.15em; }
.frasl { font-size: 1.15em; line-height: 1; }
.denom { position: relative; font-size: 0.7em; top: 0.05em; left: 0.1em; }

#wikitext .article-talk-selector a:nth-of-type(3) { color: #8e7500; }

--></style><meta http-equiv='Content-Type' content='text/html; charset=utf-8' /><link href="/wiki/uploads/favicon.png" type="image/png" rel="shortcut icon" /><link rel='preload' href='https://www.readthesequences.com/wiki/fonts/font_files/GaramondPremierProSubhead/GaramondPremierProSubhead-Medium.otf' type='font/otf' as='font' crossorigin />
<link rel='preload' href='https://www.readthesequences.com/wiki/fonts/font_files/ProximaNova/ProximaNova-Thin.otf' type='font/otf' as='font' crossorigin />
  <meta name='robots' content='index,follow' />

</head>
<body>
<!--PageText-->
<div id='wikitext'>
<div class='article-talk-selector' >
<p><a target='blank'  class='wikilink' href='https://www.readthesequences.com/Bayesians-Vs-Barbarians?action=source' title='View PmWiki source for “Bayesians vs. Barbarians”'>Source</a><a target='blank'  class='wikilink' href='https://www.readthesequences.com/Bayesians-Vs-Barbarians?action=markdown' title='View “Bayesians vs. Barbarians” in Markdown format'>Markdown</a> · <a rel='nofollow'  class='wikilink' href='https://www.readthesequences.com/Talk/Bayesians-Vs-Barbarians' title='View the Talk page for “Bayesians vs. Barbarians”'>Talk</a>
</p></div>
<div class='nav_menu' >
<p><a class='wikilink' href='https://www.readthesequences.com/'>Home</a><a class='wikilink' href='https://www.readthesequences.com/About'>About</a><a class='urllink' href='https://www.readthesequences.com/Search' rel='nofollow'>Search</a><a class='wikilink' href='https://www.readthesequences.com/Contents'>Contents</a>
</p></div>
<h1>Bayesians vs. Barbarians</h1>
<p  style='text-align: center;'> ❦
</p>
<p><a class='wikilink' href='https://www.readthesequences.com/WhyOurKindCantCooperate'>Previously</a>:
</p>
<blockquote>
<p>Let’s say we have two groups of soldiers. In group 1, the privates are ignorant of tactics and strategy; only the sergeants know anything about tactics and only the officers know anything about strategy. In group 2, everyone at all levels knows all about tactics and strategy.
</p>
<p>Should we expect group 1 to defeat group 2, because group 1 will follow orders, while everyone in group 2 comes up with <em>better ideas</em> than whatever orders they were given?
</p>
<p>In this case I have to question how much group 2 really understands about military theory, because it is an <em>elementary</em> proposition that an uncoordinated mob gets slaughtered.
</p></blockquote>
<p>Suppose that a country of rationalists is attacked by a country of <a class='wikilink' href='https://www.readthesequences.com/AreYourEnemiesInnatelyEvil'>Evil</a> Barbarians who know nothing of probability theory or decision theory.
</p>
<p>Now there’s a certain viewpoint on “rationality” or “rationalism” which would say something like this:
</p>
<p>“Obviously, the rationalists will lose. The Barbarians believe in an afterlife where they’ll be rewarded for courage; so they’ll throw themselves into battle without hesitation or remorse. Thanks to their <a class='wikilink' href='https://www.readthesequences.com/AffectiveDeathSpirals'>affective death spirals</a> around their Cause and Great Leader Bob, their warriors will obey orders, and their citizens at home will produce enthusiastically and at full capacity for the war; anyone caught skimming or holding back will be burned at the stake in accordance with Barbarian tradition. They’ll believe in each other’s goodness and hate the enemy more strongly than any sane person would, binding themselves into a tight group. Meanwhile, the rationalists will realize that there’s no conceivable reward to be had from dying in battle; they’ll wish that others would fight, but not want to fight themselves. Even if they can find soldiers, their civilians won’t be as cooperative: So long as any <em>one</em> sausage almost certainly doesn’t lead to the collapse of the war effort, they’ll want to keep that sausage for themselves, and so not contribute as much as they could. No matter how refined, elegant, civilized, productive, and nonviolent their culture was to start with, they won’t be able to resist the Barbarian invasion; sane discussion is no match for a frothing lunatic armed with a gun. In the end, the Barbarians will win because they <em>want</em> to fight, they <em>want</em> to hurt the rationalists, they <em>want</em> to conquer and their whole society is united around conquest; they care about that more than any sane person would.”
</p>
<p>War is not fun. As many, many people have found since the dawn of recorded history, as many, many people have found before the dawn of recorded history, as some community somewhere is finding out right now in some sad little country whose internal agonies don’t even make the front pages any more.
</p>
<p>War is not fun. <em>Losing</em> a war is even less fun. And it was said since the ancient times: “If thou would have peace, prepare for war.” Your opponents don’t have to believe that you’ll <em>win</em>, that you’ll conquer; but they have to believe you’ll put up enough of a fight to make it not worth their while.
</p>
<p>You perceive, then, that if it were genuinely the lot of “rationalists” to always lose in war, that I could not in good conscience advocate the widespread public adoption of “rationality.”
</p>
<p>This is probably the dirtiest topic I’ve discussed or plan to discuss here. War is not clean. Current high-tech militaries—by this I mean the US military—are unique in the overwhelmingly superior force they can bring to bear on opponents, which allows for a historically extraordinary degree of concern about enemy casualties and civilian casualties.
</p>
<p>Winning in war has not always meant tossing aside <em>all</em> morality. Wars have been won without using torture. The unfunness of war does not imply, say, that questioning the President is unpatriotic. We’re used to “war” being exploited as an excuse for bad behavior, because in recent US history that pretty much is exactly what it’s been used for…
</p>
<p>But reversed stupidity is not intelligence. And reversed evil is not intelligence either. It remains true that <em>real</em> wars cannot be won by refined politeness. If “rationalists” can’t prepare themselves for that mental shock, the Barbarians really will win; and the “rationalists”… I don’t want to say, “deserve to lose.” But they will have failed that test of their society’s existence.
</p>
<p>Let me start by disposing of the idea that, <em>in principle</em>, ideal rational agents cannot fight a war, because each of them prefers being a civilian to being a soldier.
</p>
<p>As has already been discussed at some length, I <a class='wikilink' href='https://www.readthesequences.com/NewcombsProblemAndRegretOfRationality'>one-box on Newcomb’s Problem</a>.
</p>
<p>Consistently, I do <em>not</em> believe that if an <a class='urllink' href='http://www.overcomingbias.com/2008/12/voting-kills.html' rel='nofollow'>election</a> is settled by 100,000 to 99,998 votes, that all of the voters were irrational in expending effort to go to the polling place because “my staying home would not have affected the outcome.” (Nor do I believe that if the election came out 100,000 to 99,999, then 100,000 people were <em>all</em>, individually, <em>solely responsible</em> for the outcome.)
</p>
<p>Consistently, I also hold that two rational AIs (that use my kind of decision theory), even if they had completely different utility functions and were designed by different creators, will cooperate on the <a class='wikilink' href='https://www.readthesequences.com/TheTruePrisonersDilemma'>true Prisoner’s Dilemma</a> if they have common knowledge of each other’s source code. (Or even just common knowledge of each other’s <em>rationality</em> in the appropriate sense.)
</p>
<p>Consistently, I believe that rational agents are capable of coordinating on group projects whenever the (expected probabilistic) outcome is better than it would be without such coordination. A society of agents that use my kind of decision theory, and have common knowledge of this fact, will end up at Pareto optima instead of Nash equilibria. If all rational agents agree that they are better off fighting than surrendering, they will fight the Barbarians rather than surrender.
</p>
<p>Imagine a community of self-modifying AIs who collectively prefer fighting to surrender, but individually prefer being a civilian to fighting. One solution is to run a lottery, unpredictable to any agent, to select warriors. <em>Before</em> the lottery is run, all the AIs change their code, in advance, so that if selected they will fight as a warrior in the most communally efficient possible way—even if it means calmly marching into their own death.
</p>
<p>(A reflectively consistent decision theory works the same way, only without the self-modification.)
</p>
<p>You reply: “But in the real, human world, agents are not perfectly rational, nor do they have common knowledge of each other’s source code. Cooperation in the Prisoner’s Dilemma requires certain conditions according to your decision theory (which these margins are too small to contain) and these conditions are not met in real life.”
</p>
<p>I reply: The <a class='wikilink' href='https://www.readthesequences.com/TheTruePrisonersDilemma'>pure, true Prisoner’s Dilemma</a> is incredibly rare in real life. In real life you usually have knock-on effects—what you do affects your reputation. In real life most people care to some degree about what happens to other people. And in real life you have an opportunity to set up incentive mechanisms.
</p>
<p>And in real life, I <em>do</em> think that a community of human rationalists could manage to produce soldiers willing to die to defend the community. So long as children aren’t told in school that ideal rationalists are supposed to defect against each other in the Prisoner’s Dilemma. Let it be widely believed—and I do believe it, for exactly the same reason I one-box on Newcomb’s Problem— that if people decided as individuals not to be soldiers or if soldiers decided to run away, then that is the same as deciding for the Barbarians to win. By that same theory whereby, if an election is won by 100,000 votes to 99,998 votes, it does not make sense for every voter to say “my vote made no difference.” Let it be said (for it is true) that utility functions don’t need to be solipsistic, and that a rational agent can fight to the death if they care enough about what they’re protecting. Let them not be told that rationalists should expect to lose reasonably.
</p>
<p>If this is the culture and the mores of the rationalist society, then, I think, <em>ordinary human beings</em> in that society would volunteer to be soldiers. That also seems to be built into human beings, after all. You only need to ensure that the cultural training <em>does not get in the way</em>.
</p>
<p>And if I’m wrong, and that doesn’t get you enough volunteers?
</p>
<p>Then so long as people still prefer, on the whole, fighting to surrender, they have an opportunity to set up incentive mechanisms, and avert the True Prisoner’s Dilemma.
</p>
<p>You can have lotteries for who gets elected as a warrior. Sort of like the example above with AIs changing their own code. Except that if “be reflectively consistent; do that which you would precommit to do” is not sufficient motivation for humans to obey the lottery, then…
</p>
<p>… well, in advance of the lottery actually running, we can perhaps all agree that it is a good idea to give the selectees drugs that will induce extra courage, and shoot them if they run away. Even considering that we ourselves might be selected in the lottery. Because in <em>advance</em> of the lottery, this is the general policy that gives us the highest <em>expectation</em> of survival.
</p>
<p>… like I said: Real wars = not fun, losing wars = less fun.
</p>
<p>Let’s be clear, by the way, that I’m not endorsing the draft as practiced nowadays. Those drafts are not collective attempts by a populace to move from a Nash equilibrium to a Pareto optimum. Drafts are a tool of kings playing games in need of toy soldiers. The Vietnam draftees who fled to Canada, I hold to have been in the right. But a society that considers itself too smart for kings does <em>not</em> have to be too smart to survive. Even if the Barbarian hordes are invading, and the Barbarians do practice the draft.
</p>
<p>Will rational soldiers obey orders? What if the commanding officer makes a mistake?
</p>
<p>Soldiers march. Everyone’s feet hitting the ground in the same rhythm. Even, perhaps, <a class='wikilink' href='https://www.readthesequences.com/YourPriceForJoining'>against their own inclinations</a>, since people left to themselves would walk all at separate paces. Lasers made out of people. That’s marching.
</p>
<p>If it’s possible to invent some method of group decisionmaking that is <em>superior</em> to the captain handing down orders, then a company of rational soldiers might implement that procedure. If there is no proven method better than a captain, then a company of rational soldiers commit to obey the captain, even against their own separate inclinations. And if human beings aren’t that rational… then in advance of the lottery, the general policy that gives you the highest personal expectation of survival is to shoot soldiers who disobey orders. This is not to say that those who fragged their own officers in Vietnam were in the wrong; for they could have consistently held that they preferred <em>no one</em> to participate in the draft lottery.
</p>
<p>But an uncoordinated mob gets slaughtered, and so the soldiers need <em>some</em> way of all doing the same thing at the same time in the pursuit of the same goal, even though, left to their own devices, they might march off in all directions. The orders may not come from a captain like a superior tribal chief, but unified orders have to come from <em>somewhere</em>. A society whose soldiers are too clever to obey orders is a society that is too clever to survive. Just like a society whose people are too clever to <em>be</em> soldiers. That is why I say “clever,” which I often use as a term of opprobrium, rather than “rational.”
</p>
<p>(Though I do think it’s an important question as to whether you can come up with a small-group coordination method that really genuinely in practice works better than having a leader. The more people can trust the group decision method—the more they can believe that it really is superior to people going their own way—the more coherently they can behave even in the absence of enforceable penalties for disobedience.)
</p>
<p>I say all this, even though I certainly don’t expect rationalists to take over a country any time soon, because I think that what we believe about a society of “people like us” has some reflection on what we think of ourselves. If you believe that a society of people like you would be too reasonable to survive in the long run… that’s one sort of self-image. And it’s a different sort of self-image if you think that a society of people all like you could fight the vicious Evil Barbarians and <em>win</em>—not just by dint of superior technology, but because your people care about each other and about their collective society—and because they can face the realities of war without losing themselves—and because they would calculate the group-rational thing to do and make sure it got done—and because there’s nothing in the rules of probability theory or decision theory that says you can’t sacrifice yourself for a cause—and because if you really <em>are</em> smarter than the Enemy and not just flattering yourself about that, then you should be able to exploit the blind spots that the Enemy does not allow itself to think about—and because no matter how heavily the Enemy hypes itself up before battle, you think that just maybe a coherent mind, undivided within itself, and perhaps practicing something akin to meditation or self-hypnosis, can fight as hard in practice as someone who theoretically believes they’ve got seventy-two virgins waiting for them.
</p>
<p>Then you’ll expect more of yourself and people like you operating in groups; and then you can see yourself as something more than a cultural dead end.
</p>
<p>So look at it <a class='wikilink' href='https://www.readthesequences.com/TheLogicalFallacyOfGeneralizationFromFictionalEvidence'>this way</a>: <a class='wikilink' href='https://www.readthesequences.com/TheFailuresOfEldScience'>Jeffreyssai</a> probably wouldn’t give up against the Evil Barbarians if he were fighting <em>alone</em>. A whole <em>army</em> of <em>beisutsukai</em> masters ought to be a force that <em>no one</em> would mess with. That’s the motivating vision. The question is how, exactly, that works.
</p>

<div class='bottom_nav bottom_nav_post' >
<p><a class='wikilink' href='https://www.readthesequences.com/IncrementalProgressAndTheValley'>Incremental Progress and the Valley</a>
</p>
<p><a class='wikilink' href='https://www.readthesequences.com/Contents'>Top</a>
</p>
<p><a class='wikilink' href='https://www.readthesequences.com/Book-VI-BecomingStronger'>Book</a>
</p>
<p><a class='wikilink' href='https://www.readthesequences.com/TheCraftAndTheCommunitySequence'>Sequence</a>
</p>
<p><a class='wikilink' href='https://www.readthesequences.com/BewareOfOtherOptimizing'>Beware of Other-Optimizing</a>
</p></div>
</div>

<!--PageActionFmt--><!--/PageActionFmt-->
<!--HTMLFooter-->
</body>
</html>

