<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html lang="en-US">
<head>
	<meta http-equiv="content-type" content="text/html; charset=UTF-8" />
	<link rel="canonical" href="https://www.readthesequences.com/Ethical-Injunctions" />
	<title>Ethical Injunctions  </title>
	<meta name="viewport" content="width=device-width, initial-scale=1"/>
	<link rel='stylesheet' href='https://www.readthesequences.com/wiki/pub/skins/readthesequences/skin.css' type='text/css' />
	<!--HTMLHeader--><style type='text/css'><!--.TCprogress {background-color:#5af; min-height:13px; width:13px; color:#fff; margin-left:auto; margin-right:auto;}
table.TotalCounter td {font-size:x-small; text-align:center}a[href^='http://archive.is/timegate/'] { opacity: 0.5;  }

.footnote_block_begin {
	width: 160px;
	border-bottom: 1px solid blue;
	margin-bottom: 0.5em;
}
div.footnote {
	margin: 0 3em 0.5em 0;
	padding-left: 2em;
	font-size: 0.9em;
	position: relative;
}
div.footnote .footnote-number {
	position: absolute;
	left: 0;
	width: 1.5em;
	text-align: right;
}
div.footnote .footnote-number::after {
	content: '.';
}
.num { position: relative; font-size: 0.7em; bottom: 0.5em; right: 0.1em; margin-left: 0.15em; }
.frasl { font-size: 1.15em; line-height: 1; }
.denom { position: relative; font-size: 0.7em; top: 0.05em; left: 0.1em; }

#wikitext .article-talk-selector a:nth-of-type(3) { color: #8e7500; }

--></style><meta http-equiv='Content-Type' content='text/html; charset=utf-8' /><link href="/wiki/uploads/favicon.png" type="image/png" rel="shortcut icon" /><link rel='preload' href='https://www.readthesequences.com/wiki/fonts/font_files/GaramondPremierProSubhead/GaramondPremierProSubhead-Medium.otf' type='font/otf' as='font' crossorigin />
<link rel='preload' href='https://www.readthesequences.com/wiki/fonts/font_files/ProximaNova/ProximaNova-Thin.otf' type='font/otf' as='font' crossorigin />
  <meta name='robots' content='index,follow' />

</head>
<body>
<!--PageText-->
<div id='wikitext'>
<div class='article-talk-selector' >
<p><a target='blank'  class='wikilink' href='https://www.readthesequences.com/Ethical-Injunctions?action=source' title='View PmWiki source for “Ethical Injunctions”'>Source</a><a target='blank'  class='wikilink' href='https://www.readthesequences.com/Ethical-Injunctions?action=markdown' title='View “Ethical Injunctions” in Markdown format'>Markdown</a> · <a rel='nofollow'  class='wikilink' href='https://www.readthesequences.com/Talk/Ethical-Injunctions' title='View the Talk page for “Ethical Injunctions”'>Talk</a>
</p></div>
<div class='nav_menu' >
<p><a class='wikilink' href='https://www.readthesequences.com/'>Home</a><a class='wikilink' href='https://www.readthesequences.com/About'>About</a><a class='urllink' href='https://www.readthesequences.com/Search' rel='nofollow'>Search</a><a class='wikilink' href='https://www.readthesequences.com/Contents'>Contents</a>
</p></div>
<h1>Ethical Injunctions</h1>
<p  style='text-align: center;'> ❦
</p>
<blockquote>
<p>Would you kill babies if it was the right thing to do? If no, under what circumstances would you not do the right thing to do? If yes, how right would it have to be, for how many babies?
</p>
<p class='blockquote_byline'>—<a class='urllink' href='https://www.greaterwrong.com/lw/rr/the_moral_void/' rel='nofollow'>horrible job interview question</a>
</p></blockquote>
<p>Swapping hats for a moment, I’m <em>professionally</em> intrigued by the decision theory of “things you shouldn’t do even if they seem to be the right thing to do.”
</p>
<p>Suppose we have a reflective AI, self-modifying and self-improving, at an intermediate stage in the development process. In particular, the AI’s goal system isn’t finished—the shape of its motivations is still being loaded, learned, tested, or tweaked.
</p>
<p>Yea, I have seen <a class='urllink' href='https://www.greaterwrong.com/lw/tj/dreams_of_friendliness/' rel='nofollow'>many ways to screw up an AI goal system design</a>, resulting in a decision system that decides, given its goals, that the universe ought to be tiled with <a class='wikilink' href='https://www.readthesequences.com/MagicalCategories'>tiny molecular smiley-faces</a>, or some such. Generally, these deadly suggestions also have the property that the AI will not desire its programmers to fix it. If the AI is <em>sufficiently</em> advanced—which it may be even at an intermediate stage—then the AI may also realize that deceiving the programmers, hiding the changes in its thoughts, will help transform the universe into smiley-faces.
</p>
<p>Now, from our perspective as programmers, if we <em>condition on the fact</em> that the AI has decided to hide its thoughts from the programmers, or otherwise act willfully to deceive us, then it would seem likely that some kind of unintended consequence has occurred in the goal system. We would consider it probable that the AI is <em>not</em> functioning as intended, but rather likely that we have messed up the AI’s utility function somehow. So that the AI wants to turn the universe into tiny reward-system counters, or some such, and now has a motive to hide from us.
</p>
<p>Well, suppose we’re <em>not</em> going to implement some object-level <a class='wikilink' href='https://www.readthesequences.com/FakeUtilityFunctions'>Great Idea</a> as the AI’s utility function. Instead we’re going to do something advanced and recursive—build a goal system which knows (and cares) about the programmers outside. A goal system that, via some nontrivial internal structure, “knows it’s being programmed” and “knows it’s incomplete.” Then you might be able to have and keep the rule:
</p>
<blockquote>
<p>If [I decide that] fooling my programmers is the right thing to do, execute a controlled shutdown [instead of doing the right thing to do].
</p></blockquote>
<p>And the AI would keep this rule, even through the self-modifying AI’s revisions of its own code, because, in its structurally nontrivial goal system, the present-AI understands that this decision by a future-AI <em>probably</em> indicates something defined-as-a-malfunction. Moreover, the present-AI knows that if future-AI tries to <em>evaluate</em> the utility of executing a shutdown, once this hypothetical malfunction has occurred, the future-AI will probably <em>decide</em> not to shut itself down. So the shutdown should happen unconditionally, automatically, without the goal system getting another chance to recalculate the right thing to do.
</p>
<p>I’m not going to go into the deep dark depths of the exact mathematical structure, because that would be beyond the scope of this book. Also I don’t yet know the deep dark depths of the mathematical structure. It looks like it <em>should</em> be possible, if you do things that are advanced and recursive and have nontrivial (but consistent) structure. But I <a class='wikilink' href='https://www.readthesequences.com/TheLevelAboveMine'>haven’t reached that level</a>, as yet, so for now it’s <a class='wikilink' href='https://www.readthesequences.com/DreamsOfAIDesign'>only a dream</a>.
</p>
<p>But the topic here is not advanced AI; it’s human ethics. I introduce the AI scenario to bring out more starkly the strange idea of an <em>ethical injunction</em>:
</p>
<blockquote>
<p>You should never, ever murder an innocent person who’s helped you, <em>even if it’s the right thing to do</em>; because it’s far more likely that <em>you’ve made a mistake</em>, than that murdering an innocent person who helped you is the right thing to do.
</p></blockquote>
<p>Sound reasonable?
</p>
<p>During World War II, it became necessary to destroy Germany’s supply of deuterium, a neutron moderator, in order to block their attempts to achieve a fission chain reaction. Their supply of deuterium was coming at this point from a captured facility in Norway. A shipment of heavy water was on board a Norwegian ferry ship, the <a class='urllink' href='http://en.wikipedia.org/wiki/SF%20Hydro' rel='nofollow'>SF <em>Hydro</em></a>. Knut Haukelid and three others had slipped on board the ferry in order to sabotage it, when the saboteurs were discovered by the ferry watchman. Haukelid told him that they were escaping the Gestapo, and the watchman immediately agreed to overlook their presence. Haukelid “considered warning their benefactor but decided that might endanger the mission and only thanked him and shook his hand.”<a id='citation1'></a><a href='#footnote1'>1</a> So the civilian ferry <em>Hydro</em> sank in the deepest part of the lake, with eighteen dead and twenty-nine survivors. Some of the Norwegian rescuers felt that the German soldiers present should be left to drown, but this attitude did not prevail, and four Germans were rescued. And that was, effectively, the end of the Nazi atomic weapons program.
</p>
<p>Good move? Bad move? Germany <em>very likely</em> wouldn’t have gotten the Bomb anyway… I hope with absolute desperation that I never get faced by a choice like that, but in the end, I can’t say a word against it.
</p>
<p>On the other hand, when it comes to the rule:
</p>
<blockquote>
<p>Never try to deceive yourself, or offer a reason to believe other than probable truth; because even if you come up with an amazing clever reason, it’s more likely that you’ve made a mistake than that you have a reasonable expectation of this being a net benefit in the long run.
</p></blockquote>
<p>Then I really <em>don’t</em> know of anyone who’s knowingly been faced with an exception. There are times when you try to convince yourself “I’m not hiding any Jews in my basement” before you talk to the Gestapo officer. But then you do still know the truth, you’re just trying to create something like an alternative self that exists in your imagination, a facade to talk to the Gestapo officer.
</p>
<p>But to really believe something that isn’t true? I don’t know if there was ever anyone for whom that was <em>knowably</em> a good idea. I’m sure that there have been many many times in human history, where person <em>X</em> was better off with false belief <em>Y</em>. And by the same token, there is always some set of winning lottery numbers in every drawing. It’s <em>knowing which lottery ticket will win</em> that is the epistemically difficult part, like <em>X</em> knowing when they’re better off with a false belief.
</p>
<p>Self-deceptions are the worst kind of black swan bets, much worse than lies, because without knowing the true state of affairs, you can’t even guess at what the penalty will be for your self-deception. They only have to blow up once to undo all the good they ever did. One single time when you pray to God after discovering a lump, instead of going to a doctor. That’s all it takes to undo a life. All the happiness that the warm thought of an afterlife ever produced in humanity, has now been more than cancelled by the failure of humanity to institute systematic cryonic preservations after liquid nitrogen became cheap to manufacture. And I don’t think that anyone ever had that sort of failure in mind as a possible blowup, when they said, “But we need religious beliefs to cushion the fear of death.” That’s what black swan bets are all about—the unexpected blowup.
</p>
<p>Maybe you even get away with one or two black swan bets—they don’t get you <em>every</em> time. So you do it again, and then the blowup comes and cancels out every benefit and then some. That’s what black swan bets are all about.
</p>
<p>Thus the difficulty of knowing when it’s safe to believe a lie (assuming you can even manage that much mental contortion in the first place)—part of the nature of black swan bets is that you don’t see the bullet that kills you; and since our perceptions just seem like the way the world is, it looks like there is no bullet, period.
</p>
<p>So I would say that there is an ethical injunction against self-deception. I call this an “ethical injunction” not so much because it’s a matter of interpersonal morality (although it is), but because it’s a rule that guards you from your own cleverness—an override against the temptation to do what seems like the right thing.
</p>
<p>So now we have two kinds of situation that can support an “ethical injunction,” a rule not to do something even when it’s the right thing to do. (That is, you refrain “even when your brain has computed it’s the right thing to do,” but this will just <em>seem like</em> “the right thing to do.”)
</p>
<p>First, being human and <a class='wikilink' href='https://www.readthesequences.com/EndsDontJustifyMeansAmongHumans'>running on corrupted hardware</a>, we may <a class='urllink' href='https://www.greaterwrong.com/lw/uu/why_does_power_corrupt/' rel='nofollow'>generalize classes of situation</a> where when you say e.g. “It’s time to rob a few banks for the greater good,” we deem it more likely that you’ve been corrupted than that this is really the case. (Note that we’re not prohibiting it from <em>ever</em> being the case in <em>reality</em>, but we’re questioning the <em>epistemic</em> state where you’re <em>justified in trusting</em> your own calculation that this is the right thing to do—fair lottery tickets can win, but you can’t justifiably buy them.)
</p>
<p>Second, history may teach us that certain classes of action are black swan bets, that is, they sometimes blow up bigtime for reasons not in the decider’s model. So even when we calculate within the model that something seems like the right thing to do, we apply the further knowledge of the black swan problem to arrive at an injunction against it.
</p>
<p>But surely… if one is <em>aware of these reasons</em>… then one can simply redo the calculation, taking them into account. So we can rob banks if it seems like the right thing to do <em>after taking into account</em> the problem of corrupted hardware and black swan blowups. That’s the rational course, right?
</p>
<p>There’s a number of replies I could give to that.
</p>
<p>I’ll start by saying that this is a prime example of the sort of thinking I have in mind, when I warn aspiring rationalists to beware of cleverness.
</p>
<p>I’ll also note that I wouldn’t want an attempted Friendly AI that had just decided that the Earth ought to be transformed into paperclips, to assess whether this was a reasonable thing to do in light of all the various warnings it had received against it. I would want it to undergo an automatic controlled shutdown. Who says that meta-reasoning is immune from corruption?
</p>
<p>I could mention the important times that my naive, idealistic ethical inhibitions have <a class='urllink' href='https://www.greaterwrong.com/lw/uz/protected_from_myself/' rel='nofollow'>protected me from <em>myself</em></a>, and placed me in a recoverable position, or helped start the recovery, from very deep mistakes I had no clue I was making. And I could ask whether I’ve really advanced so much, and whether it would really be all that wise, to remove the protections that saved me before.
</p>
<p>Yet even so… “Am I still dumber than my ethics?” is a question whose answer isn’t <em>automatically</em> “Yes.”
</p>
<p>There are obvious silly things here that you shouldn’t do; for example, you shouldn’t wait until you’re really tempted, and <em>then</em> try to figure out if you’re smarter than your ethics on that particular occasion.
</p>
<p>But in general—there’s only so much power that can vest in what your parents told you not to do. One shouldn’t underestimate the power. Smart people debated historical lessons in the course of forging the Enlightenment ethics that much of Western culture draws upon; and some subcultures, like scientific academia, or science-fiction fandom, draw on those ethics more directly. But even so the power of the past is bounded.
</p>
<p>And in fact…
</p>
<p>I’ve had to make my ethics <em>much stricter</em> than what my parents and <a class='wikilink' href='https://www.readthesequences.com/RaisedInTechnophilia'>Jerry Pournelle</a> and <a class='wikilink' href='https://www.readthesequences.com/NoSafeDefenseNotEvenScience'>Richard Feynman</a> told me not to do.
</p>
<p>Funny thing, how when people seem to think they’re smarter than their ethics, they argue for <em>less</em> strictness rather than <em>more</em> strictness. I mean, when you think about how much more complicated the modern world is…
</p>
<p>And along the same lines, the ones who come to me and say, “You should lie about the intelligence explosion, because that way you can get more people to support you; it’s the rational thing to do, for the greater good”—these ones seem to have <em>no idea</em> of the risks.
</p>
<p>They don’t mention the problem of running on corrupted hardware. They don’t mention the idea that lies have to be recursively protected from all the truths and all the truthfinding techniques that threaten them. They don’t mention that honest ways have a simplicity that dishonest ways often lack. They don’t talk about black swan bets. They don’t talk about the terrible nakedness of discarding the last defense you have against yourself, and trying to survive on raw calculation.
</p>
<p>I am reasonably sure that this is because they have <em>no clue</em> about any of these things.
</p>
<p>If you’ve truly understood the reason and the rhythm behind ethics, then one major sign is that, augmented by this newfound knowledge, you <em>don’t do</em> those things that previously seemed like ethical transgressions. Only now you know why.
</p>
<p>Someone who just looks at one or two reasons behind ethics, and says, “Okay, I’ve understood that, so now I’ll take it into account consciously, and therefore I have no more need of ethical inhibitions”—this one is behaving more like a stereotype than a real rationalist. The world isn’t simple and pure and clean, so you can’t just take the ethics you were raised with and trust them. But that pretense of Vulcan logic, where you think you’re just going to compute everything correctly once you’ve got one or two abstract insights—that doesn’t work in real life either.
</p>
<p>As for those who, having figured out <em>none</em> of this, think themselves smarter than their ethics: Ha.
</p>
<p>And as for those who previously thought themselves smarter than their ethics, but who hadn’t conceived of all these elements behind ethical injunctions “in so many words” until they ran across this essay, and who <em>now</em> think themselves smarter than their ethics, because they’re going to take all this into account from now on: Double ha.
</p>
<p>I have seen many people struggling to excuse themselves from their ethics. Always the modification is toward lenience, never to be more strict. And I am stunned by the speed and the lightness with which they strive to abandon their protections. Hobbes said, “I don’t know what’s worse, the fact that everyone’s got a price, or the fact that their price is so low.” So very low the price, so very eager they are to be bought. They don’t <a class='wikilink' href='https://www.readthesequences.com/MotivatedStoppingAndMotivatedContinuation'>look twice</a> and then a <a class='wikilink' href='https://www.readthesequences.com/TheThirdAlternative'>third time</a> for alternatives, before deciding that they have no option left but to transgress—though they may look very grave and solemn when they say it. They abandon their ethics at the very first opportunity. “Where there’s a will to failure, obstacles can be found.” The will to fail at ethics seems very strong, in some people.
</p>
<p>I don’t know if I can endorse absolute ethical injunctions that bind over all possible epistemic states of a human brain. The universe isn’t kind enough for me to trust that. (Though an ethical injunction against self-deception, for example, does seem to me to have tremendous force. I’ve seen many people arguing for the <a class='wikilink' href='https://www.readthesequences.com/DarkSideEpistemology'>Dark Side</a>, and none of them seem aware of the network risks or the black-swan risks of self-deception.) If, someday, I attempt to shape a (reflectively consistent) injunction within a self-modifying AI, it will only be after working out the math, because that is so totally not the sort of thing you could get away with doing via an ad-hoc patch.
</p>
<p>But I will say this much:
</p>
<p><em>I am completely unimpressed with the knowledge, the reasoning, and the overall level of those folk who have eagerly come to me, and said in grave tones</em>, “It’s rational to do unethical thing X because it will have benefit Y. ”
</p>

<div class='footnotes' >
<p><a id='footnote1'></a> <span class='footnote'> Richard Rhodes, <em>The Making of the Atomic Bomb</em> (New York: Simon &amp; Schuster, 1986). </span><span class='back_to_citation_link'><a href='#citation1'>↩︎</a></span>
</p></div>
<div class='bottom_nav bottom_nav_post' >
<p><a class='wikilink' href='https://www.readthesequences.com/EndsDontJustifyMeansAmongHumans'>Ends Don’t Justify Means (Among Humans)</a>
</p>
<p><a class='wikilink' href='https://www.readthesequences.com/Contents'>Top</a>
</p>
<p><a class='wikilink' href='https://www.readthesequences.com/Book-V-MereGoodness'>Book</a>
</p>
<p><a class='wikilink' href='https://www.readthesequences.com/QuantifiedHumanismSequence'>Sequence</a>
</p>
<p><a class='wikilink' href='https://www.readthesequences.com/SomethingToProtect'>Something to Protect</a>
</p></div>
</div>

<!--PageActionFmt--><!--/PageActionFmt-->
<!--HTMLFooter-->
</body>
</html>

